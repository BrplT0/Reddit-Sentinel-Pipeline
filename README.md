# üåç Reddit Sentinel Pipeline

> A high-throughput data pipeline that scrapes, processes, and analyzes Reddit comments to create an interactive happiness visualization dashboard.

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![PRAW](https://img.shields.io/badge/PRAW-7.7+-orange.svg)](https://praw.readthedocs.io/)
[![Transformers](https://img.shields.io/badge/%F0%9F%A4%97%20Transformers-4.x-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

---

## üìä Project Overview

This project leverages Reddit's vast user-generated content to analyze and visualize global happiness trends. By processing **hundreds of thousands of comments** weekly, we create a sentiment-based interactive dashboard showing which regions are happiest based on social media discourse.

### üéØ Current Status: **Production Ready & Deployed**

The complete pipeline is **production-ready** with cloud automation, featuring:

- üó∫Ô∏è **Interactive Choropleth Map** - Global happiness heatmap
- üìà **Time Series Analysis** - Track happiness trends over time
- üèÜ **Top/Bottom Rankings** - Happiest and unhappiest countries
- üåê **150+ Countries** - Comprehensive global coverage
- ‚òÅÔ∏è **Cloud GPU Analysis** - Automated Kaggle cloud processing
- üêã **Docker Deployment** - Production-ready containerization
- ‚è∞ **Automated Scheduling** - Weekly cron jobs

---

## ‚ú® Key Features

- üöÄ **High-Throughput Scraper** - Optimized API usage with capacity of **~350,000 comments/hour**
- üéØ **Geographic Filtering** - Target specific regions or analyze global data
- ‚öôÔ∏è **Triple Backend Options** - CPU multiprocessing, local GPU, or **Kaggle cloud GPU automation**
- ü§ñ **Dual NLP Models** - Choose between XLM-RoBERTa (accurate) or DistilBERT (fast)
- üîß **Highly Configurable** - Fine-tune scraping depth, approval thresholds, and time windows
- ‚ôªÔ∏è **Smart Caching** - Re-runnable pipeline skips redundant processing
- üì¶ **Efficient Archival** - Historical data compressed and preserved
- üêã **Docker Support** - One-command deployment with docker-compose
- ‚è∞ **Automated Scheduling** - Weekly cron jobs for hands-free operation

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Reddit API (PRAW)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Subreddit Checker  ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Validates 197 countries)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ    Post Scraper     ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Scrapes posts from last 7 days)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Comment Scraper    ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Capacity: ~350K/hour)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ   Preprocessing     ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (RegEx cleaning, bot/spam filtering)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Sentiment Analysis  ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (CPU/GPU/Kaggle Cloud)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Data Aggregation   ‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Country-level statistics)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  Streamlit Dashboard‚îÇ  ‚úÖ COMPLETE
      ‚îÇ                     ‚îÇ  (Interactive visualization)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

### Core Pipeline
- **Python 3.12** - Core language
- **PRAW 7.7+** - Reddit API Wrapper
- **Pandas** - Data manipulation and ETL
- **NumPy** - Data chunking for multiprocessing

### NLP & Performance
- **Hugging Face `transformers`** - XLM-RoBERTa model
- **PyTorch (`torch`)** - ML backend (CPU/GPU)
- **`multiprocessing`** - Parallel processing on CPU

### Visualization
- **Streamlit** - Interactive web dashboard
- **Plotly** - Choropleth & time series visualization

### Configuration & Security
- **`configparser`** - Settings management
- **`python-dotenv`** - Environment variable handling
- **`logging`** - Multiprocessing-safe logging

### Deployment & Automation
- **Docker** - Containerization
- **Docker Compose** - Multi-service orchestration
- **Cron** - Automated scheduling
- **Kaggle API** - Cloud GPU automation

---

## üìÇ Project Structure

```
Reddit-Sentinel-Pipeline/
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ subreddits.csv            # üîí Populated list (Ignored by Git)
‚îÇ   ‚îî‚îÄ‚îÄ subreddits.template.csv   # ‚úÖ Public template (Tracked by Git)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.ini                # ‚úÖ Public settings (Tracked by Git)
‚îú‚îÄ‚îÄ data/                          # üîí Generated outputs (Ignored by Git)
‚îÇ   ‚îú‚îÄ‚îÄ archived/                  # Compressed historical data
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/                 # Current dashboard data
‚îÇ   ‚îú‚îÄ‚îÄ logs/                      # Pipeline execution logs
‚îÇ   ‚îú‚îÄ‚îÄ processed/                 # Cleaned and analyzed data
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Raw scraped comments
‚îÇ   ‚îî‚îÄ‚îÄ weekly_scrapings/          # Weekly scraping results
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/                # ‚úÖ NLP and Aggregation
‚îÇ   ‚îú‚îÄ‚îÄ checkers/                 # ‚úÖ Subreddit validation
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # ‚úÖ Core utilities (Logger, Config)
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                 # ‚úÖ Data collection
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # ‚úÖ Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ dashboard/                # ‚úÖ Streamlit visualization
‚îú‚îÄ‚îÄ .dockerignore                 # Docker ignore patterns
‚îú‚îÄ‚îÄ .env.example                  # ‚úÖ API Key template
‚îú‚îÄ‚îÄ Dockerfile                    # Docker container definition
‚îú‚îÄ‚îÄ docker-compose.yml            # Multi-service orchestration
‚îú‚îÄ‚îÄ main.py                       # Main execution pipeline
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Setup & Installation

> **Note:** The project automatically creates the `/data` directory structure on first run. You only need to configure API keys and the subreddit list.

### Prerequisites
- **Python 3.11 or 3.12** (3.12 for CPU-only, 3.11 recommended for GPU/CUDA)
- Reddit API credentials
- **16GB+ RAM** (Recommended for CPU multiprocessing on large datasets)
- **(Optional)** Kaggle account for cloud GPU automation
- **(Optional)** Docker for containerized deployment

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/BrplT0/Reddit-Sentinel-Pipeline.git
cd Reddit-Sentinel-Pipeline

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install core dependencies
pip install -r requirements.txt
# This installs the CPU-only version of PyTorch by default.
```

### Configuration (Required Steps)

**1. Create `.env` file (API Secrets)**

Rename `.env.example` to `.env` and add your Reddit API keys.

```bash
# On Linux/Mac:
mv .env.example .env

# On Windows:
ren .env.example .env

# Now edit the .env file with your credentials
```

**2. Create `subreddits.csv` (Input Data)**

Rename `subreddits.template.csv` to `subreddits.csv`. The template includes example subreddits.

```bash
# On Linux/Mac:
mv assets/subreddits.template.csv assets/subreddits.csv

# On Windows (CMD):
cd assets
ren subreddits.template.csv subreddits.csv
cd ..

# Edit the file if you want to add/remove subreddits
```

**3. Configure `config.ini` (Pipeline Settings)**

Open `config/config.ini` to customize the pipeline behavior. The configuration file is divided into multiple sections:

```ini
[global]
# Geographic scope: "all", "world", "asia", "africa", "europe", 
# "south_america", "north_america", "oceania"
category = europe

# Scraping lookback period (days)
comment_max_days = 7

[check_subreddits]
# Minimum comments required for subreddit approval
comment_approve_point = 1000

# Minimum subscribers required
sub_approve_point = 100000

[reddit_post_scraper]
# Max posts per subreddit
post_limit = 150

# Minimum comments per post to process
post_comment_approve_limit = 50

[reddit_comment_scraper]
# Comment depth limit (32 = optimal balance)
# 0 = top-level only, None = all (causes rate limits)
comment_link_limit = 32

[analysis]
# Hardware selection: "cpu", "gpu", or "kaggle_auto"
device_type = kaggle_auto

# CPU cores (only used if device_type = cpu)
cpu_cores = 4

# Model selection: "roberta" (accurate, 1.6GB) or "distilbert" (faster, 0.5GB)
model_name = roberta
```

> **Note:** The `/data` directory structure will be **automatically created** by the project when you first run `main.py`.

### (Optional) Setup for Kaggle Cloud GPU Automation

**NEW!** For the best performance without owning a GPU:

**Step 1: Create Kaggle Account & Get API Credentials**

1. Create a Kaggle account at [kaggle.com](https://www.kaggle.com)
2. Go to your profile: Click your avatar (top right) ‚Üí **Account** or visit [kaggle.com/settings](https://www.kaggle.com/settings)
3. Scroll down to **API** section
4. Click **"Create Legacy API Token"** - This downloads `kaggle.json`



**Step 2: Install Kaggle API Credentials**

```bash
# Linux/Mac
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Windows (PowerShell)
mkdir $env:USERPROFILE\.kaggle -Force
Move-Item $env:USERPROFILE\Downloads\kaggle.json $env:USERPROFILE\.kaggle\

# Windows (Command Prompt)
mkdir %USERPROFILE%\.kaggle
move %USERPROFILE%\Downloads\kaggle.json %USERPROFILE%\.kaggle\
```

**Step 3: Verify Installation**

```bash
pip install kaggle
kaggle datasets list  # Should show public datasets
```

**Step 4: Update `config.ini`**

```ini
[analysis]
device_type = kaggle_auto
model_name = roberta  # Kaggle GPUs handle XLM-RoBERTa easily
```

**How it works:**
- Automatically uploads preprocessed data to Kaggle Dataset
- Creates and executes a Kaggle Notebook with P100 GPU
- Downloads sentiment results when complete
- Zero local GPU required!

> **Important Notes:**
> - The `kaggle.json` file contains your API credentials - keep it secure
> - Kaggle free tier provides 30 GPU hours per week
> - First run may take 15-20 min (dataset upload + GPU warmup)
> - Subsequent runs are faster (~10-15 min)

### (Optional) Setup for Local GPU (NVIDIA/CUDA)

If you want to use a local NVIDIA GPU for faster analysis:

1. Ensure you are using **Python 3.11 or 3.12** (3.11 recommended for CUDA compatibility)
2. Uninstall the CPU-only `torch`:

```bash
pip uninstall torch
```

3. Install CUDA-enabled PyTorch (e.g., CUDA 12.1):

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

4. Update `config.ini`:

```ini
[analysis]
device_type = gpu
model_name = distilbert  # Recommended for GPUs with <4GB VRAM
```

> **GPU Requirements:** Minimum 3GB VRAM recommended. XLM-RoBERTa requires ~1.6GB, DistilBERT requires ~0.5GB.

---

## üêã Docker Deployment

### Quick Start with Docker Compose

The easiest way to deploy the entire stack:

```bash
# 1. Ensure .env and assets/subreddits.csv are configured
# 2. Start all services
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Services

**Dashboard Service** (`reddit-happiness-dashboard`)
- Runs Streamlit on port 8501
- Access at `http://localhost:8501`
- Auto-restarts on failure

**Pipeline Service** (`reddit-happiness-pipeline`)
- Runs main.py for manual execution
- Interactive mode enabled

**Scheduler Service** (`reddit-happiness-scheduler`)
- Automated weekly execution (Tuesday 2 AM)
- Uses cron for scheduling
- Timezone-aware

### Manual Docker Build

```bash
# Build image
docker build -t reddit-happiness .

# Run dashboard only
docker run -p 8501:8501 -v ./data:/app/data -v ./config:/app/config --env-file .env reddit-happiness streamlit run src/dashboard/app.py

# Run pipeline manually
docker run -v ./data:/app/data -v ./config:/app/config --env-file .env reddit-happiness python main.py
```

---

## üöÄ Usage

### Run the Complete Pipeline

```bash
python main.py
```

The script is **re-runnable**. If it finds already processed data from today (`cleaned_comments.csv`), it will skip the ~20-min scraping/cleaning steps and jump straight to the analysis.

### Run Dashboard Only

```bash
streamlit run src/dashboard/app.py
```

### With Docker

```bash
# Start dashboard (24/7 service)
docker-compose up -d dashboard

# Run pipeline manually
docker-compose run pipeline

# Enable automated weekly runs
docker-compose up -d scheduler
```

---

## üìà Performance Benchmarks

### Scraping Performance
- **~350,000 comments/hour** with optimized API usage (tested: 131K in 25 min)
- **Comment depth:** 32 levels captures 99.9% of data without rate limits
- **Geographic filtering:** Reduces processing time by targeting specific regions

### NLP Analysis Performance

**Kaggle Cloud GPU (P100) - RECOMMENDED**
- **XLM-RoBERTa:**
  + 100,000+ comments: ~10-15 minutes
  + Automatic orchestration via API
  + Zero local hardware requirements
  + Free tier: 30 hours/week GPU time

**CPU Mode (Ryzen 5 8400F, 6 Cores)**
- **XLM-RoBERTa:**
  + 10,000 comments (Single-Core, `batch_size=64`): ~6 min 17 sec
  + 14,000 comments (`Pool(4)`): ~21 minutes
- **DistilBERT:** (Estimated 2-3x faster than RoBERTa)

> *Note: Benchmarks on Windows with small datasets. Multiprocessing overhead is significant for small volumes. Linux performance with larger datasets will show greater parallelization benefits.*

**Local GPU Mode**
- **NVIDIA MX350 (2GB VRAM):**
  + XLM-RoBERTa: ‚ùå `OutOfMemoryError` (model 1.6GB + batch data exceeds VRAM)
  + DistilBERT: ‚úÖ Expected to work (0.5GB model footprint)
- **Recommended GPU:** 3GB+ VRAM (e.g., T4, RTX 3060) for XLM-RoBERTa

### Model Comparison

| Model | VRAM Usage | CPU Speed | Accuracy | Recommended For |
|-------|------------|-----------|----------|-----------------|
| **XLM-RoBERTa** | ~1.6GB | Baseline | High | Kaggle cloud, powerful GPUs |
| **DistilBERT** | ~0.5GB | 2-3x faster | Good | CPU mode, limited VDS |

### Backend Comparison

| Backend | Speed | Cost | Best For |
|---------|-------|------|----------|
| **Kaggle Cloud** | ‚ö°‚ö°‚ö° Fastest | üÜì Free | Large datasets, no local GPU |
| **Local GPU** | ‚ö°‚ö° Very Fast | üí∞ Hardware cost | Frequent runs, privacy needs |
| **CPU Mode** | ‚ö° Moderate | üÜì Free | Small datasets, testing |

---

## üéì Key Learnings & Challenges

### Technical Achievements

**1. Cloud GPU Automation (NEW!)**
- Implemented full Kaggle API integration for automated cloud processing
- Built `KaggleAutomator` class for dataset upload, notebook execution, and result retrieval
- Achieved 10x+ speedup vs local CPU without hardware investment
- Handles authentication, error recovery, and status monitoring

**2. Production Deployment Infrastructure**
- Dockerized complete stack with multi-service orchestration
- Implemented automated weekly scheduling with cron in containers
- Configured timezone-aware scheduling for reliable execution
- Volume mounting for persistent data across container restarts

**3. Advanced Configuration System**
- Built a comprehensive multi-section config supporting geographic filtering, threshold tuning, and hardware selection
- Implemented triple-backend support (CPU/GPU/Kaggle) with single config switch
- Designed optimal comment depth limit (32 levels) to capture 99.9% of data without hitting API rate limits

**4. Hardware Optimization**
- Diagnosed GPU memory limitations: 2GB VRAM insufficient for XLM-RoBERTa (1.6GB model + batch data)
- Validated DistilBERT as efficient alternative (0.5GB VRAM) for resource-constrained environments
- Optimized CPU multiprocessing as reliable path for production VDS deployment
- Discovered Kaggle cloud as ideal solution for GPU-level performance without hardware

**5. Robust Error Handling**
- `TypeError: NoneType`: Implemented `.fillna()` before processing API responses
- `IndexError: 512`: Added `truncation=True, max_length=512` to handle token limits
- `AssertionError: Torch not compiled`: Resolved CUDA vs CPU package compatibility

**6. Pipeline Efficiency**
- Idempotent design: Script checks for existing data, skips 20+ min scraping if present
- Balanced scraping strategy: `comment_link_limit=32` avoids rate limits while maximizing data capture
- Multiprocessing logging: Custom logger with `MainProcess` check prevents duplicate entries

**7. Atomic Writes for Live Dashboard**
- Solved data conflict (race condition) between the weekly pipeline (Writer) and the 7/24 dashboard (Reader)
- Implemented `shutil.move` to atomically swap `.tmp` files, ensuring the dashboard never reads a partially written CSV and never crashes

---

## üîÆ Roadmap

### ‚úÖ Completed
- [x] Subreddit validation system
- [x] High-throughput post & comment scraper
- [x] Preprocessing pipeline (cleaning, filtering)
- [x] Multilingual sentiment analysis (XLM-RoBERTa)
- [x] Country-level data aggregation
- [x] Interactive Streamlit dashboard with choropleth map, time series, and rankings
- [x] **Docker containerization**
- [x] **Automated weekly scheduling with cron**
- [x] **Kaggle cloud GPU automation**

### üöÄ Current Focus: Production Optimization
- [ ] VDS deployment with reverse proxy (Nginx)
- [ ] SSL/HTTPS setup with Let's Encrypt
- [ ] Production monitoring and alerting
- [ ] Backup and disaster recovery system

### üîú Future Enhancements
- [ ] Custom frontend development (replacing Streamlit)
- [ ] Database migration evaluation (PostgreSQL or MongoDB)
- [ ] Email notification system for pipeline failures
- [ ] Advanced time series forecasting with ML
- [ ] Multi-language UI support
- [ ] RESTful API endpoint for data access
- [ ] Mobile-responsive frontend
- [ ] Historical data comparison tools
- [ ] Sentiment trend predictions with LSTM/Transformer
- [ ] Regional analysis (sub-national level)
- [ ] Real-time streaming analysis

---

## üêõ Known Issues & Limitations

- **No Test Suite**: Lacks automated unit tests (e.g., `pytest`)
- **Manual Monitoring**: Production monitoring system not yet implemented
- **CSV Storage**: May need database migration for long-term scalability
- **Kaggle Rate Limits**: Free tier limited to 30 GPU hours/week

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Berat Polat**

- GitHub: [@BrplT0](https://github.com/BrplT0)
- LinkedIn: [Berat Polat](https://www.linkedin.com/in/berat-polat-923093249)

---

### ‚≠ê If you find this project interesting, please consider starring it!

**Status**: üöÄ Production Ready | üêã Dockerized | ‚òÅÔ∏è Cloud-Enabled | üìä Pipeline Complete

Built with ‚ù§Ô∏è and ‚òï | Learning by doing
